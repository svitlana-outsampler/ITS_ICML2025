# todo
- [x ] add the ids in the metric json files
- [x] modify the mistral.py script for appending generated data to data.json (instead of erasing it)
- [x] change the number presentation with scaling and rounding (1.23456 -> 12) 
- [x] adapt the convert.py script for computing diagnostics on each of the three sentences: cosines + llm evaluation (thus four metrics)
- [x] check the dataset against another model (Qwen3 30b)
- [x] discuss results
- [] modify OU class for separating random paramters generation and time series generation
- [] add time series paramaters to the data.jsonl file
- [] program the possibility to recompute the "ground truth" without regenerating the data
- [] curate: detect wrong extrema and replace by ground truth
- [] adjust the ground truth until around 10% of samples has to be verified
- [] train
- [] evaluate
- [] train a model with images (not numbers)
 

# llm_ts

This project intends to use small LLM to interpret time series data. As is, small LLMs are not very powerful in this task. 
It is expected that distilling large models into smaller ones will improve the performance.

This README provides information on the whole training and inference pipeline. 

## Installation

The training should work on NVIDIA GPUs, but we use the AMD MI210 cards.

create a python env:
```bash
python -m venv venv
source venv/bin/activate
```

install the requirements:
```bash
pip install -r requirements.txt
```

This is for training and inference on the AMD MI210 cards. For NVIDIA card, the line
```bash
--extra-index-url https://download.pytorch.org/whl/rocm5.7
```
should be removed from the requirements.txt file.

## Generating the dataset

We use the pixtral-large model of Mistral AI. The API key must be given in a shell variable by
```bash
export MISTRAL_API_KEY=<your_api_key>
```
In the line
```python
chat = Mistral(dryrun = False)
```
the dryrun parameter should be set to False to generate the dataset.

In the python environment just type

```bash
python mistral.py
```

This will generate a dataset of time series in the directory `dataset`. The number of samples can be changed directly in the script. Each time series is described by a .dat file containing the data, a .png file containing the plot, and a .txt file containing the description (assumed to be of gold value) generated by Pixtral.
The values and text are also stored in a json file `data.json`.

The pixtral model is queried with both images and numerical data. If only numerical data are provided to the large Mistral model, the results are not good at all.

## Data curation

I test several "truth" sentences to compare with. Surprisingly a difficult task is to find truth about the trend that agrees with the LLM. In practice the LLM gives good description and the NLI score detects errors (for around 7% of the samples) that are almost all irrelevant. For the noise it is ok, the ambiguity is sufficient so that the sentences do not contradict. For the extrema description, it was also easy: if the NLI score is bad (4% of the samples), I simply replace in the dataset the LLM sentence by the hand made generated description. 

On the trend description, the NLI approach detect 15 contradictions for 220 samples. Most of them are irrelevant. There are some edge cases (less than 2%) that can be kept because even if the description is not perfect it cannot be considered as completely false.
On the noise description, the NLI approach detect 28 contradictions for 220 samples, which can be ignored.
On the extrema description, the NLI approach detect 10 contradictions for 220 samples. These are true contradictions. They are fixed so that in the end there is no contradiction in the extrema description.

## Training the model

First convert the dataset to a format that can be used by the training script. This is done by the script `convert.py` that creates a single file `test_jsonl.jsonl` containing  a list of pairs of reference prompt/answers.

The training can last a couple of hours. For this it is necessary to launch it batch mode. This is done by connecting to the server `gaya.math.unistra.fr`. Go to the installation directory of `llm_ts` 
and type

```bash
sbatch go.slurm
```

Adjust the file `go.slurm` to your needs (for instance, the number of GPUs and the time limit)
The training is programmed in the `qwen_train.py` script.

To monitor the progression, here are a few useful commands:
```bash
squeue -u <your_username>
scontrol show job <job_id>
scancel <job_id>
tail -f train.out
tail -f train.err
```
The computations are periodically saved, and could be stopped and restarted but this is not yet implemented.

The training files are saved in the directory corresponding to the model name.


## Testing the model
At the end of the training, the model is tested against 10% of the dataset (a part that is not used for training). A semantic embedding is computed for the answers and the gold answers. The cosine similarity is then computed, and the average is displayed. This metric is not very performant for detecting contradictions.

The tests can also be inspected in the files of model directory. The files are `evaluation_avant.json` (before) and `evaluation_checkpoint-xxx.json`, xxx being the number of the checkpoint.

A more friendly display is obtained with the script `ts_check_training.py` that displays the results on the test set in a more readable way. The `ts_check_training.py` must be launched in the model directory. The command is `python ../ts_check_training.py <evaluation_checkpoint-xx.json>`. If the name of the JSON file is omitted then the checker ask a remote LLM to generate other outputs that are compared with the gold answers. For tihs to work, an API key must be provided before with the command `export TEXTSYNTH_API_KEY=...`.
The pictures are saved in the directory `plotdiag`.

## Using the model

### 1 · Convert a Hugging Face model to **GGUF**

> GGUF is the on‑disk format that `llama.cpp` understands. Everything else—quantisation, adapters, server—starts with having a `.gguf` file.

#### 1.1 Prerequisites

```bash
# clone and build llama.cpp (or grab the binaries)
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp && make -j          # or follow the build guide for your OS

# Python bits for the converter scripts
pip install -r requirements.txt  # transformers, peft, safetensors, …
```

#### 1.2 Option A — native `llama.cpp` converters

| scenario                              | command                                                                                                         |
| ------------------------------------- | --------------------------------------------------------------------------------------------------------------- |
| **Plain model** (float16)             | `python llama.cpp/convert_hf_to_gguf.py meta-llama/Llama-3-8b --outfile llama3‑8b‑f16.gguf --outtype f16`       |
| **Same model, quantised to Q4\_K\_M** | `python llama.cpp/convert_hf_to_gguf.py meta-llama/Llama-3-8b --outfile llama3‑8b‑q4_K_M.gguf --outtype q4_K_M` |
| **Convert LoRA adapter only**         | `python llama.cpp/convert_lora_to_gguf.py ./my_adapter --outfile my_adapter‑f16.gguf --outtype f16`             |
| **Merge LoRA first**                  | 1) merge with PEFT (see below) → temp dir 2) run `convert_hf_to_gguf.py` on the merged checkpoint               |

`--outtype` accepts every quantiser listed by `llama-quantize` (`f32`, `f16`, `q8_0`, `q6_K`, `q4_K_M`, `q3_K` …).  Choose the smallest type your hardware can comfortably run; Q4\_K\_M is a common sweet‑spot. ([github.com](https://github.com/ggml-org/llama.cpp/discussions/2948))

##### Quick one‑liner to merge a LoRA in place

```bash
python - <<'PY'
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import sys, pathlib, torch
base, lora, out = sys.argv[1:4]
model = AutoModelForCausalLM.from_pretrained(base, torch_dtype='auto', device_map='cpu')
model = PeftModel.from_pretrained(model, lora)
model = model.merge_and_unload()
(outp:=pathlib.Path(out)).mkdir(parents=True, exist_ok=True)
model.save_pretrained(outp, safe_serialization=True)
AutoTokenizer.from_pretrained(base).save_pretrained(outp)
PY  meta-llama/Llama-3-8b  ./my_adapter  ./merged

python llama.cpp/convert_hf_to_gguf.py ./merged --outfile llama3‑8b‑merged‑q6_K.gguf --outtype q6_K
```

#### 1.3 Option B — all‑in‑one helper **`hf2gguf.py`**

If you prefer a single entry‑point that decides whether you are converting a plain model or `base + LoRA`, use the wrapper script from the previous section:

```bash
python hf2gguf.py --base meta-llama/Llama-3-8b \
                  --lora TheBloke/Samantha-8B-LoRA \
                  --merge-lora \
                  --quant q4_K_M \
                  --outfile llama3-samantha-q4k.gguf
```

The script exposes three strategies:

| strategy          | output files                 | when to pick                                          |
| ----------------- | ---------------------------- | ----------------------------------------------------- |
| `merge` (default) | **one** merged `.gguf`       | you only ever plan to use this adapter with this base |
| `separate`        | `base.gguf` + `adapter.gguf` | space‑efficient; swap adapters at runtime             |
| *plain*           | just `base.gguf`             | no adapter at all                                     |

---

### 2 · Running inference

#### 2.1 `llama-cli` – quick tests & interactive shells

```bash
# single GGUF
llama-cli -m llama3‑8b‑q4_K_M.gguf -p "What is 2+2?"

# base + one adapter
llama-cli -m llama3‑8b‑q4_K_M.gguf --lora Samantha‑8B‑F16.gguf \
          -p "Translate this into pirate slang: Ahoy, friend!"

# chain multiple adapters (≥ v0.2)
llama-cli -m llama3‑8b‑q4_K_M.gguf \
          --lora style‑pirate.gguf \
          --lora domain‑medical.gguf \
          -p "..."
```

The adapter flags (`--lora`, `--lora-scaled`) were added in Autumn 2024 and allow stacking or scaling per adapter.  Examples from the official discussion: ([github.com](https://github.com/ggml-org/llama.cpp/discussions/10123))

#### 2.2 `llama-server` – OpenAI‑compatible REST endpoint

```bash
# one liner
llama-server -m llama3‑8b‑q4_K_M.gguf --lora Samantha‑8B‑F16.gguf
```

* Repeat `--lora` for each adapter you want pre‑loaded.
* Use `--lora-init-without-apply` to start the server with adapters loaded but not yet active; apply/remove them on‑the‑fly via the `POST /lora-adapters` API (hot‑reload). ([github.com](https://github.com/ggml-org/llama.cpp/discussions/10123))

#### 2.3 `llama-run` – minimal one‑shot runner

`llama-run` is a tiny wrapper useful for scripting and CI pipelines:

```bash
llama-run -m llama3‑8b‑q4_K_M.gguf -p "Write a haiku about frogs" -n 128
```

The utility behaves like `llama-cli` but prints only the generated text, making it easy to pipe into other tools. ([github.com](https://github.com/ggml-org/llama.cpp?utm_source=chatgpt.com))

---

### 3 · Managing **multiple GGUFs**

| approach                      |  pros                                                    | cons                                                                  | typical command                                              |
| ----------------------------- | -------------------------------------------------------- | --------------------------------------------------------------------- | ------------------------------------------------------------ |
| **Merged** (single file)      | everything in one place; works with every client         | larger file; can’t swap adapters                                      | *convert via `--merge-lora` then* `llama-cli -m merged.gguf` |
| **Separate** (base + adapter) | tiny adapters; combine freely; hot‑reload in server mode | each run needs extra `--lora` flags; adapter quantisation must ≤ base | `llama-cli -m base.q4_K_M.gguf --lora my_adapter.f16.gguf`   |

> **Tip :** The adapter’s quantisation **does not need to match** the base, but it must not be *higher* precision than the base (e.g. F16 adapter on Q4 base is fine; Q4 adapter on F16 base will degrade quality).

---

### 4 · Troubleshooting cheatsheet

* **“not enough VRAM”** → choose a smaller `--outtype` (`q4_K_M` instead of `q6_K`, `q3_K_S` instead of `q4_K_M`).
* **Tokenizer mismatch** after merging LoRA → always export the tokenizer from the *base*, not the adapter.
* **Slow first token** on GPU → pass `--gpu-layers N` where *N* fits your VRAM; leaving everything on CPU is often faster than partial GPU offload on weak cards.

---

Refer to the [llama.cpp README](https://github.com/ggml-org/llama.cpp) for the full list of runtime flags—context window (`-c`), sampling parameters, hardware back‑ends, and more. ([github.com](https://github.com/ggml-org/llama.cpp))


## Results

In order to generate the dataset, we first synthetically generate 220 time series of an Orstein-Uhlenbeck process with 128 samples each. The values are rescaled between 0 and 99 and rounded to nearest integer. This is done for improving the LLM understanding of the time series. It is known that too much digit precision can be confusing for the LLM.
The Python function for generating one time series is the following:
```python
def generate(self, imagename="ou_process.png", filename="ou_process.dat"):
    ... to do : extract relevant code
```
Then the synthetic time series are analyzed with the model "mistral-large-latest" from Mistral AI (https://mistral.ai/) (May 2025).  The prompt used for the analysis is the following:
```
/think Describe the time series in three sentences. First sentence: describe trend (increasing/decreasing/flat). Second sentence: noise intensity (low/medium/high). Third sentence: approximate localisation of global maximum (beginning/middle/end) and global minimum (beginning/middle/end).
Put the description in a JSON format with the following pattern
{ \"trend\": <sentence1>,\n  \"noise\": <sentence2>,\n  \"extrema\": <sentence3> }.\n Series: [00, 01, 02, 03, 04, 04, 06, ... <rest of the series>...]
```
The Mistral output is considered as the "gold" output.

In the end the dataset contains 200 samples, 90% of which are used for training, the ohter 10% are used for testing.

We train on approximately 13 epochs and 160 steps, until the loss stops decreasing. The chosen loss is the cross-entropy loss evaluated on the test dataset.

Once the finie tuning is done, we evaluate the model on the test dataset for several metrics described below.

The cosine similarity is computed between the gold and the generated output with the sentence-transformers model `sentence-transformers/all-MiniLM-L6-v2`. This metric is good at detecting semantic similarities, however it is not good at detecting contradictions.

We provide in the two tables below the evolution of the cosine similarity during the training for each small model: Qwen2.5 0.5B and Qwen2.5 1.5B.


Therefore we also compute a NLI (Natural Language Inference) score with the model `roberta-large-mnli`. The gold ouput is taken as hypothesis and the small model output is taken as premise. Then the NLI predicts if the premise entails the hypothesis, contradicts it or is neutral. For each sample and each of the three diagnostic sentence ("trend", "noise", "extrema"), we attribute 0 to contradiction, 0.5 to neutral and 1 to entailment and compute the average score over the samples. At the beginning of the training, the small LLM is often even unable to generate a conforming JSON output. In this case, we decide to convert the output to empty sentences. This generally produces a "neutral" diagnostic of the NLI model and thus a score of 0.5.

 It was not possible to human check all the Mistral outputs. In order to measure the confidence that we can have for the produced results, we therefore ask to another large LLM (Qwen3 30 A3B) the same questions and compare the answers thanks to the NLI scores. The average scores for the three sentences are:

 sentence, NLI score
"trend", 0.75
"noise", 0.6
"extrema", 0.675

todo: this must be discussed because while the NLI score seem bad the sentences are generally in agreement.

From the tables, we observe that initially the small LLMs are not able to generate a conforming JSON output. In addition the sentence diagnostics are often wrong. The LLM is generally not even able to detect increasing or decreasing trends. After a few steps, the LLM is able to generate a conforming JSON output. The similarity score mainly detects this capacity, but is not able to detect the validity of the diagnostics and possible contradictions with the "gold" output. The NLI score increases until the end of training and measure the semantic and logical coherence of the generated output. In conclusion, there is clear knowledge distillation from the large LLM to the small LLM.


### Qwen2.5-1.5B-Instruct

| Step          | 0    | 20   | 40   | 60   | 80   | 100  | 120  | 140  | 160   |
| ------------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ----  |
| Similarity    | 0.50 | 0.98 | 0.98 | 0.98 | 0.98 | 0.98 | 0.98 | 0.98 | 0.98  |
| NLI "trend"   | 0.50 | xxxx | xxxx | xxxx | xxxx | xxxx | xxxx | xxxx | 0.875 |
| NLI "noise"   | 0.50 | xxxx | xxxx | xxxx | xxxx | xxxx | xxxx | xxxx | 0.75  |
| NLI "extrema" | 0.50 | xxxx | xxxx | xxxx | xxxx | xxxx | xxxx | xxxx | 0.875 |






### Qwen2.5-0.5B-Instruct

| Step          | 0     | 20    | 40    | 60    | 80    | 100   | 120   | 140   | 160   |
| ------------- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- |
| Similarity    | 0.62  | 0.98  | xxxx  | xxxx  | 0.98  | 0.98  | 0.98  | 0.98  | 0.98  |
| NLI "trend"   | 0.5   | 0.575 | xxxx  | xxxx  | xxxx  | 0.9   | 0.83  | 0.83  | 0.875 |
| NLI "noise"   | 0.5   | 0.725 | xxxx  | xxxx  | xxxx  | 0.65  | 0.625 | 0.55  | 0.675 |
| NLI "extrema" | 0.5   | 0.4   | xxxx  | xxxx  | xxxx  | 0.75  | 0.875 | 0.78  | 0.775 |