# todo
- [x ] add the ids in the metric json files
- [x] modify the mistral.py script for appending generated data to data.json (instead of erasing it)
- [x] change the number presentation with scaling and rounding (1.23456 -> 12) 
- [x] adapt the convert.py script for computing diagnostics on each of the three sentences: cosines + llm evaluation (thus four metrics)
- [x] check the dataset against another model (Qwen3 30b)
- [x] discuss results
- [] modify OU class for separating random paramters generation and time series generation
- [] add time series paramaters to the data.jsonl file
- [] program the possibility to recompute the "ground truth" without regenerating the data
- [] curate: detect wrong extrema and replace by ground truth
- [] adjust the ground truth until around 10% of samples has to be verified
- [] train
- [] evaluate
- [] train a model with images (not numbers)
 

# llm_ts

This project intends to use small LLM to interpret time series data. As is, small LLMs are not very powerful in this task. 
It is expected that distilling large models into smaller ones will improve the performance.

This README provides information on the whole training and inference pipeline. 

## Installation

The training should work on NVIDIA GPUs, but we use the AMD MI210 cards.

create a python env:
```bash
python -m venv venv
source venv/bin/activate
```

install the requirements:
```bash
pip install -r requirements.txt
```

This is for training and inference on the AMD MI210 cards. For NVIDIA card, the line
```bash
--extra-index-url https://download.pytorch.org/whl/rocm5.7
```
should be removed from the requirements.txt file.

## Generating the dataset

We use the pixtral-large model of Mistral AI. The API key must be given in a shell variable by
```bash
export MISTRAL_API_KEY=<your_api_key>
```
In the line
```python
chat = Mistral(dryrun = False)
```
the dryrun parameter should be set to False to generate the dataset.

In the python environment just type

```bash
python mistral.py
```

This will generate a dataset of time series in the directory `dataset`. The number of samples can be changed directly in the script. Each time series is described by a .dat file containing the data, a .png file containing the plot, and a .txt file containing the description (assumed to be of gold value) generated by Pixtral.
The values and text are also stored in a json file `data.json`.

The pixtral model is queried with both images and numerical data. If only numerical data are provided to the large Mistral model, the results are not good at all.

## Training the model

First convert the dataset to a format that can be used by the training script. This is done by the script `convert.py` that creates a single file `test_jsonl.jsonl` containing  a list of pairs of reference prompt/answers.

The training can last a couple of hours. For this it is necessary to launch it batch mode. This is done by connecting to the server `gaya.math.unistra.fr`. Go to the installation directory of `llm_ts` 
and type

```bash
sbatch go.slurm
```

Adjust the file `go.slurm` to your needs (for instance, the number of GPUs and the time limit)
The training is programmed in the `qwen_train.py` script.

To monitor the progression, here are a few useful commands:
```bash
squeue -u <your_username>
scontrol show job <job_id>
scancel <job_id>
tail -f train.out
tail -f train.err
```
The computations are periodically saved, and could be stopped and restarted but this is not yet implemented.

The training files are saved in the directory corresponding to the model name.


## Testing the model
At the end of the training, the model is tested against 10% of the dataset (a part that is not used for training). A semantic embedding is computed for the answers and the gold answers. The cosine similarity is then computed, and the average is displayed. This metric is not very performant for detecting contradictions.

The tests can also be inspected in the files of model directory. The files are `evaluation_avant.json` (before) and `evaluation_checkpoint-xxx.json`, xxx being the number of the checkpoint.

A more friendly display is obtained with the script `ts_check_training.py` that displays the results on the test set in a more readable way. The `ts_check_training.py` must be launched in the model directory. The command is `python ../ts_check_training.py <evaluation_checkpoint-xx.json>`. If the name of the JSON file is omitted then the checker ask a remote LLM to generate other outputs that are compared with the gold answers. For tihs to work, an API key must be provided before with the command `export TEXTSYNTH_API_KEY=...`.
The pictures are saved in the directory `plotdiag`.

## Using the model

Conversion to `.gguf` format.

TO DO 

Inference: TO DO

## Results

In order to generate the dataset, we first synthetically generate 200-1000 time series of an Orstein-Uhlenbeck process with 128 samples each. The values are rescaled betwenn 0 and 99 and rounded to nearest integer. This is done for improving the LLM understanding of the time series. It is known that too much digit precision can be confusing for the LLM.
The Python function for generating one time series is the following:
```python
def generate(self, imagename="ou_process.png", filename="ou_process.dat"):
    ... to do : extract relevant code
```
Then the synthetic time series are analyzed with the model "mistral-large-latest" from Mistral AI (https://mistral.ai/) (May 2025).  The prompt used for the analysis is the following:
```
Describe the time series in three sentences. First sentence: describe increasing/decreasing/flat trend. Second sentence: possible presence and intensity of noise. Third sentence: describe local and global extrema.\nPut the description in a JSON format with the following pattern\n{ \"trend\": <sentence1>,\n  \"noise\": <sentence2>,\n  \"extrema\": <sentence3> }.\n Series: [00, 01, 02, 03, 04, 04, 06, ... <rest of the series>...]
```
The Mistral output is considered as the "gold" output.

In the end the dataset contains 200-1000 samples, 90% of which are used for training, the ohter 10% are used for testing.

We train on approximately 13 epochs and 160 steps, until the loss stops decreasing. The chosen loss is the cross-entropy loss evaluated on the test dataset.

Once the finie tuning is done, we evaluate the model on the test dataset for several metrics described below.

The cosine similarity is computed between the gold and the generated output with the sentence-transformers model `sentence-transformers/all-MiniLM-L6-v2`. This metric is good at detecting semantic similarities, however it is not good at detecting contradictions.

We provide in the two tables below the evolution of the cosine similarity during the training for each small model: Qwen2.5 0.5B and Qwen2.5 1.5B.


Therefore we also compute a NLI (Natural Language Inference) score with the model `roberta-large-mnli`. The gold ouput is taken as hypothesis and the small model output is taken as premise. Then the NLI predicts if the premise entails the hypothesis, contradicts it or is neutral. For each sample and each of the three diagnostic sentence ("trend", "noise", "extrema"), we attribute 0 to contradiction, 0.5 to neutral and 1 to entailment and compute the average score over the samples. At the beginning of the training, the small LLM is often even unable to generate a conforming JSON output. In this case, we decide to convert the output to empty sentences. This generally produces a "neutral" diagnostic of the NLI model and thus a score of 0.5.

 It was not possible to human check all the Mistral outputs. In order to measure the confidence that we can have for the produced results, we therefore ask to another large LLM (Qwen3 30 A3B) the same questions and compare the answers thanks to the NLI scores. The average scores for the three sentences are:

 sentence, NLI score
"trend", 0.75
"noise", 0.6
"extrema", 0.675

todo: this must be discussed because while the NLI score seem bad the sentences are generally in agreement.

From the tables, we observe that initially the small LLMs are not able to generate a conforming JSON output. In addition the sentence diagnostics are often wrong. The LLM is generally not even able to detect increasing or decreasing trends. After a few steps, the LLM is able to generate a conforming JSON output. The similarity score mainly detects this capacity, but is not able to detect the validity of the diagnostics and possible contradictions with the "gold" output. The NLI score increases until the end of training and measure the semantic and logical coherence of the generated output. In conclusion, there is clear knowledge distillation from the large LLM to the small LLM.


### Qwen2.5-1.5B-Instruct

| Step          | 0    | 20   | 40    | 60    | 80    | 100   | 120   | 140   | 160   |
| ------------- | ---- | ---- | ----- | ----- | ----- | ----- | ----- | ----- | ----- |
| Similarity    | 0.66 | 0.95 | 0.95  | 0.96  | 0.96  | 0.96  | 0.95  | 0.96  | 0.96  |
| NLI "trend"   | 0.45 | 0.50 | 0.475 | 0.70  | 0.70  | 0.825 | 0.825 | 0.75  | 0.875 |
| NLI "noise"   | 0.50 | 0.70 | 0.725 | 0.725 | 0.875 | 0.925 | 0.775 | 0.775 | 0.90  |
| NLI "extrema" | 0.50 | 0.45 | 0.575 | 0.575 | 0.60  | 0.575 | 0.55  | 0.55  | 0.65  |





### Qwen2.5-0.5B-Instruct

| Step          | 0    | 20    | 40     | 60    | 80    | 100   | 120   | 140   | 160   |
| ------------- | ---- | ----- | -----  | ----- | ----- | ----- | ----- | ----- | ----- |
| Similarity    | 0.549 | 0.958  | 0.961  | 0.961  | 0.946  | 0.955  | 0.954  | 0.957  | 0.952 |
| NLI "trend"   | 0.5   | 0.575 | 0.6    | 0.625  | 0.65  | 0.8  | 0.775  | 0.75  | 0.7 |
| NLI "noise"   | 0.5   | 0.725 | 0.775  | 0.8    | 0.675  | 0.85  | 0.875  | 0.85  | 0.85 |
| NLI "extrema" | 0.5   | 0.4   | 0.5    | 0.5    | 0.35  | 0.5  | 0.475  | 0.55  | 0.775 |


