# todo
- [ ] modify the mistral.py script for appending generated data to data.json (instead of erasing it)
- [] launch it for generating 1000 samples
- [] adapt the convert.py script for computing diagnostics on each of the three sentences: cosines + llm evaluation (thus four metrics)
- [] train on 200 steps and 1000 samples
- [] discuss results

# llm_ts

This project intends to use small LLM to interpret time series data. As is, small LLMs are not very powerful in this task. 
It is expected that distilling large models into smaller ones will improve the performance.

This README provides information on the whole training and inference pipeline. 

## Installation

The training should work on NVIDIA GPUs, but we use the AMD MI210 cards.

create a python env:
```bash
python -m venv venv
source venv/bin/activate
```

install the requirements:
```bash
pip install -r requirements.txt
```

This is for training and inference on the AMD MI210 cards. For NVIDIA card, the line
```bash
--extra-index-url https://download.pytorch.org/whl/rocm5.7
```
should be removed from the requirements.txt file.

## Generating the dataset

We use the pixtral-large model of Mistral AI. The API key must be given in a shell variable by
```bash
export MISTRAL_API_KEY=<your_api_key>
```
In the line
```python
chat = Mistral(dryrun = False)
```
the dryrun parameter should be set to False to generate the dataset.

In the python environment just type

```bash
python mistral.py
```

This will generate a dataset of 50 time series in the directory `dataset`. The value of 50 can be changed directly in the script. Each time series is described by a .dat file containing the data, a .png file containing the plot, and a .txt file containing the description (assumed to be of gold value) generated by Pixtral.
The values and text are also stored in a json file `data.json`.

## Training the model

First convert the dataset to a format that can be used by the training script. This is done by the script `convert.py` that creates a single file `test_jsonl.jsonl` containing 50 (or more) pairs of reference prompt/answers.

The training can last a couple of hours. For this it is necessary to launch it batch mode. This is done by connecting to the server `gaya.math.unistra.fr`. Go to the installation directory of `llm_ts` 
and type

```bash
sbatch go.slurm
```

Adjust the file `go.slurm` to your needs (for instance, the number of GPUs and the time limit)

To monitor the progression, here are a few useful commands:
```bash
squeue -u <your_username>
scontrol show job <job_id>
scancel <job_id>
tail -f train.out
tail -f train.err
```
The computations are periodically saved, and could be stopped and restarted but this is not yet implemented.

The training files are saved in the directory corresponding to the model name.


## Testing the model
At the end of the training, the model is tested against 10% of the dataset (a part that is not used for training). A semantic embedding is computed for the answers and the gold answers. The cosine similarity is then computed, and the average is displayed.
The tests can also be inspected in the files `evaluation_avant.json` (before) and `evaluation_apres.json` (after).

A more friendly is obtained with the script `check_training.py` that displays the results on the test set in a more readable way. The `check_training.py` must be launched in the model directory.
The pictures are saved in the directory `plotdiag`.

## Using the model

Conversion to `.gguf` format.

TO DO 

Inference: TO DO

## Results

The dataset contains 200 samples, 90% are used for training, 10% are used for testing.

We train on 5 epochs and 100 steps.

The cosine similarity is computed with the sentence-transformers model `sentence-transformers/all-MiniLM-L6-v2`.

In the `check_training.py` script, the cosine similarity is computed with the sentence-transformers model `sentence-transformers/paraphrase-MiniLM-L6-v2`, which is supposed to be more accurate (but in practice, I am not convinced)

### Qwen2.5-0.5B-Instruct

cosine similarity before training:  0.5749
cosine similarity after training:   0.8391


### Qwen2.5-1.5B-Instruct

cosine similarity before training:  0.7461
cosine similarity after training:  0.7959


The improvment seems to be better for the small model.
