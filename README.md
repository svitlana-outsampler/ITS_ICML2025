# llm_ts

This project intends to use small LLM to interpret time series data. As is, small LLMs are not very powerful in this task. 
It is expected that distilling large models into smaller ones will improve the performance.

This README provides information on the whole training and inference pipeline. 

## Installation

The training should work on NVIDIA GPUs, but we use the AMD MI210 cards.

create a python env:
```bash
python -m venv venv
source venv/bin/activate
```

install the requirements:
```bash
pip install -r requirements.txt
```

This is for training and inference on the AMD MI210 cards. For NVIDIA card, the line
```bash
--extra-index-url https://download.pytorch.org/whl/rocm5.7
```
should be removed from the requirements.txt file.

## Generating the dataset

We use the pixtral-large model of Mistral AI. The API key must be given in a shell variable by
```bash
export MISTRAL_API_KEY=<your_api_key>
```
In the line
```python
chat = Mistral(dryrun = False)
```
the dryrun parameter should be set to False to generate the dataset.

In the python environment just type

```bash
python mistral.py
```

This will generate a dataset of 50 time series in the directory `dataset`. The value of 50 can be changed directly in the script. Each time series is described by a .dat file containing the data, a .png file containing the plot, and a .txt file containing the description (assumed to be of gold value) generated by Pixtral.
The values and text are also stored in a json file `data.json`.

## Training the model

First convert the dataset to a format that can be used by the training script. This is done by the script `convert.py` that creates a single file `test_jsonl.jsonl` containing 50 pairs of reference prompt/answers.

The training can last a couple of hours. For this it is necessary to launch it batch mode. This is done by connecting to the server `gaya.math.unistra.fr`. Go the installation directory of `llm_ts` 
and type

```bash
sbatch go.slurm
```

Adjust the file `go.slurm` to your needs (for instance, the number of GPUs and the time limit)

To monitor the progression, here are a few useful commands:
```bash
squeue -u <your_username>
scontrol show job <job_id>
scancel <job_id>
tail -f slurm-<job_id>.out
```
The computations are periodically saved, and could be stopped and restarted but this is not yet implemented.


## Testing the model
At the end of the training, the model is tested against 10% of the dataset (a part that is not used for training). A semantic embedding is computed for the answers and the gold answers. The cosine similarity is then computed, and the average is displayed.
The tests can also be inspected in the files `evaluation_avant.json` (before) and `evaluation_apres.json` (after).

## Using the model

Conversion to `.gguf` format.

TO DO 

Inference: TO DO
