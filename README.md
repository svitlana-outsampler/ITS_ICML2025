# llm_ts

This project post-trains small LLMs (such Qwen instruction-trained models) to interpret time series data. 

This README provides information on the whole training and inference pipeline. 

## Installation

The training should work on NVIDIA GPUs, but we use the AMD MI210 cards.

create a python env:
```bash
python -m venv venv
source venv/bin/activate
```

install the requirements:
```bash
pip install -r requirements.txt
```

This is for training and inference on the AMD MI210 cards. For NVIDIA card, the line
```bash
--extra-index-url https://download.pytorch.org/whl/rocm5.7
```
should be removed from the requirements.txt file.

## Generating the dataset

We use the pixtral-large model of Mistral AI. The API key must be given in a shell variable by
```bash
export MISTRAL_API_KEY=<your_api_key>
```
In the line
```python
chat = Mistral(dryrun = False)
```
the dryrun parameter should be set to False to generate the dataset.

In the python environment just type

```bash
python mistral.py
```

This will generate a dataset of time series in the directory `dataset`. The number of samples can be changed directly in the script. Each time series is described by a .dat file containing the data, a .png file containing the plot, and a .txt file containing the description (assumed to be of gold value) generated by Pixtral.
The values and text are also stored in a json file `data.json`. The pixtral model is queried with both images and numerical data. If only numerical data are provided to the large Mistral model, the results are not good at all.

## Training the model

First convert the dataset to a format that can be used by the training script. This is done by the script `convert.py` that creates a single file `test_jsonl.jsonl` containing  a list of pairs of reference prompt/answers.
The training is programmed in the `qwen_train.py` script.

To monitor the progression, here are a few useful commands:
```bash
squeue -u <your_username>
scontrol show job <job_id>
scancel <job_id>
tail -f train.out
tail -f train.err
```
The training files are saved in the directory corresponding to the model name.


## Testing the model
At the end of the training, the model is tested against 10% of the dataset (a part that is not used for training). A semantic embedding is computed for the answers and the gold answers. The cosine similarity is then computed, and the average is displayed. This metric is not very performant for detecting contradictions.

The tests can also be inspected in the files of model directory. The files are `evaluation_avant.json` (before) and `evaluation_checkpoint-xxx.json`, xxx being the number of the checkpoint.

A more friendly display is obtained with the script `ts_check_training.py` that displays the results on the test set in a more readable way. The `ts_check_training.py` must be launched in the model directory. The command is `python ../ts_check_training.py <evaluation_checkpoint-xx.json>`. If the name of the JSON file is omitted then the checker ask a remote LLM to generate other outputs that are compared with the gold answers. For tihs to work, an API key must be provided before with the command `export TEXTSYNTH_API_KEY=...`.
The pictures are saved in the directory `plotdiag`.
